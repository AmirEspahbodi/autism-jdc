# Training Configuration for JDC Framework

# Model Selection
model_name: "meta-llama/Meta-Llama-3-8B-Instruct"
# Alternative: "mistralai/Mistral-7B-Instruct-v0.2"

# Data Paths
train_data_path: "data/train.jsonl"
val_data_path: "data/val.jsonl"  # Optional: set to null if no validation data
max_seq_length: 2048

# Training Hyperparameters
num_train_epochs: 3
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
gradient_accumulation_steps: 4  # Effective batch size = 4 * 4 = 16

# Learning Rate Schedule
learning_rate: 2.0e-4
warmup_ratio: 0.03
lr_scheduler_type: "cosine"
weight_decay: 0.01

# Optimization
optim: "paged_adamw_32bit"  # Memory-efficient optimizer
gradient_checkpointing: true
max_grad_norm: 1.0

# Logging and Saving
output_dir: "./checkpoints/jdc_llama3_8b"
logging_steps: 10
save_steps: 100
eval_steps: 100
save_total_limit: 3  # Keep only last 3 checkpoints

# WandB Integration (Optional)
use_wandb: false
wandb_project: "jdc-ableism-detection"
wandb_run_name: "llama3-8b-qlora"

# Device Configuration
device_map: "auto"

# Quantization Settings
quantization:
  load_in_4bit: true
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

# LoRA Configuration
lora:
  r: 16                    # LoRA rank (higher = more parameters)
  lora_alpha: 32           # Scaling factor (typically 2*r)
  lora_dropout: 0.05       # Dropout for regularization
  bias: "none"             # Don't train bias terms
  task_type: "CAUSAL_LM"
  target_modules:          # Modules to apply LoRA
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
